Command:        mpirun -np 96 --bind-to l3cache --map-by ppr:24:numa -x LD_LIBRARY_PATH -x PATH -x PWD /fsx/build/openmpi_gcc/CMAQ_v533/CCTM/scripts/BLD_CCTM_v533_gcc_profile/CCTM_v533.exe
Resources:      1 node (96 physical, 96 logical cores per node)
Memory:         370 GiB per node
Tasks:          96 processes
Machine:        queue1-dy-compute-resource-1-1
Start time:     Fri Dec 16 06:31:45 2022
Total time:     2250 seconds (about 38 minutes)
Full path:      /fsx/build/openmpi_gcc/CMAQ_v533/CCTM/scripts/BLD_CCTM_v533_gcc_profile

Summary: CCTM_v533.exe is Compute-bound in this configuration
Compute:                                     70.6% |======|
MPI:                                         26.0% |==|
I/O:                                          3.4% ||
This application run was Compute-bound. A breakdown of this time and advice for investigating further is in the CPU section below. 
As little time is spent in MPI calls, this code may also benefit from running at larger scales.

CPU:
A breakdown of the 70.6% CPU time:
Scalar numeric ops:                          28.5% |==|
Vector numeric ops:                           5.6% ||
Memory accesses:                             65.9% |======|
The per-core performance is memory-bound. Use a profiler to identify time-consuming loops and check their cache performance.
Little time is spent in vectorized instructions. Check the compiler's vectorization advice to see why key loops could not be vectorized.

MPI:
A breakdown of the 26.0% MPI time:
Time in collective calls:                    62.6% |=====|
Time in point-to-point calls:                37.4% |===|
Effective process collective rate:            1.96 kB/s
Effective process point-to-point rate:        50.9 MB/s
Most of the time is spent in collective calls with a very low transfer rate. This suggests load imbalance is causing synchronization overhead; use an MPI profiler to investigate.
The point-to-point transfer rate is low. This can be caused by inefficient message sizes, such as many small messages, or by imbalanced workloads causing processes to wait.

I/O:
A breakdown of the 3.4% I/O time:
Time in reads:                               99.5% |=========|
Time in writes:                               0.5% ||
Effective process read rate:                  6.61 GB/s
Effective process write rate:                 2.83 GB/s
Most of the time is spent in read operations with a high effective transfer rate. It may be possible to achieve faster effective transfer rates using asynchronous file operations.

Threads:
A breakdown of how multiple threads were used:
Computation:                                100.0% |=========|
Synchronization:                              0.0% |
Physical core utilization:                   96.6% |=========|
System load:                                 99.5% |=========|
Thread usage appears to be well-optimized. Check the CPU breakdown for advice on further improving performance.

Memory:
Per-process memory usage may also affect scaling:
Mean process memory usage:                    1.25 GiB
Peak process memory usage:                    1.60 GiB
Peak node memory usage:                      39.0% |===|
The peak node memory usage is low. Running with fewer MPI processes and more data on each process may be more efficient.

Energy:
A breakdown of how energy was used:
CPU:                                      not supported
System:                                   not supported
Mean node power:                          not supported
Peak node power:                              0.00 W
Energy metrics are not available on this system.
CPU metrics are not supported (no intel_rapl module)

